{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "* Detect the text on the actual image.\n",
    "* Detect the text on the white background image.\n",
    "* Check if there is size difference.\n",
    "* Make the bounding boxes the same size by expanding the smaller one.\n",
    "* Than find the mapping function according to the actual image canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023/07/06 15:59:25] ppocr DEBUG: Namespace(alpha=1.0, benchmark=False, beta=1.0, cls_batch_num=6, cls_image_shape='3, 48, 192', cls_model_dir='/Users/busraasan/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_thresh=0.9, cpu_threads=10, crop_res_save_dir='./output', det=True, det_algorithm='DB', det_box_type='quad', det_db_box_thresh=0.6, det_db_score_mode='fast', det_db_thresh=0.3, det_db_unclip_ratio=1.5, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_east_score_thresh=0.8, det_limit_side_len=960, det_limit_type='max', det_model_dir='/Users/busraasan/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, det_pse_thresh=0, det_sast_nms_thresh=0.2, det_sast_score_thresh=0.5, draw_img_save_dir='./inference_results', drop_score=0.5, e2e_algorithm='PGNet', e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_limit_side_len=768, e2e_limit_type='max', e2e_model_dir=None, e2e_pgnet_mode='fast', e2e_pgnet_score_thresh=0.5, e2e_pgnet_valid_set='totaltext', enable_mkldnn=False, fourier_degree=5, gpu_mem=500, help='==SUPPRESS==', image_dir=None, image_orientation=False, ir_optim=True, kie_algorithm='LayoutXLM', label_list=['0', '180'], lang='en', layout=True, layout_dict_path=None, layout_model_dir=None, layout_nms_threshold=0.5, layout_score_threshold=0.5, max_batch_size=10, max_text_length=25, merge_no_span_structure=True, min_subgraph_size=15, mode='structure', ocr=True, ocr_order_method=None, ocr_version='PP-OCRv3', output='./output', page_num=0, precision='fp32', process_id=0, re_model_dir=None, rec=True, rec_algorithm='SVTR_LCNet', rec_batch_num=6, rec_char_dict_path='/Users/busraasan/miniconda3/envs/causalml-py38/lib/python3.8/site-packages/paddleocr/ppocr/utils/en_dict.txt', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_model_dir='/Users/busraasan/.paddleocr/whl/rec/en/en_PP-OCRv3_rec_infer', recovery=False, save_crop_res=False, save_log_path='./log_output/', scales=[8, 16, 32], ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ser_model_dir=None, show_log=True, sr_batch_num=1, sr_image_shape='3, 32, 128', sr_model_dir=None, structure_version='PP-StructureV2', table=True, table_algorithm='TableAttn', table_char_dict_path=None, table_max_len=488, table_model_dir=None, total_process_num=1, type='ocr', use_angle_cls=True, use_dilation=False, use_gpu=False, use_mp=False, use_npu=False, use_onnx=False, use_pdf2docx_api=False, use_pdserving=False, use_space_char=True, use_tensorrt=False, use_visual_backbone=True, use_xpu=False, vis_font_path='./doc/fonts/simfang.ttf', warmup=False)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "from utils import *\n",
    "\n",
    "from paddleocr import PaddleOCR,draw_ocr\n",
    "\n",
    "from PIL import Image, ImageFont\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib.colors import hsv_to_rgb, rgb_to_hsv\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "# Paddleocr supports Chinese, English, French, German, Korean and Japanese.\n",
    "# You can set the parameter `lang` as `ch`, `en`, `fr`, `german`, `korean`, `japan`\n",
    "# to switch the language model in order.\n",
    "def compose_paragraphs(text_bboxes, text_palettes):\n",
    "\n",
    "        '''\n",
    "            Compose text data into paragraphs.\n",
    "            Return: Grouped indices of detected text elements.\n",
    "        '''\n",
    "        num_text_boxes = len(text_palettes)\n",
    "        print(num_text_boxes)\n",
    "        composed_text_idxs = [[0]]\n",
    "        for i in range(num_text_boxes-1):\n",
    "            palette1 = text_palettes[i]\n",
    "            palette2 = text_palettes[i+1]\n",
    "            if np.array_equal(palette1, palette2):\n",
    "                bbox1 = text_bboxes[i]\n",
    "                bbox2 = text_bboxes[i+1]\n",
    "                height1 = bbox1[0][1] - bbox1[3][1]\n",
    "                height2 = bbox2[0][1] - bbox2[3][1]\n",
    "                if abs(bbox1[0][1]-bbox2[0][1]) <= abs(height1)+30:\n",
    "                    if i != 0 and i not in composed_text_idxs[-1]:\n",
    "                        composed_text_idxs.append([i])\n",
    "                    composed_text_idxs[-1].append(i+1)\n",
    "                else:\n",
    "                    if i != 0 and i not in composed_text_idxs[-1] and [i] not in composed_text_idxs:\n",
    "                        composed_text_idxs.append([i])\n",
    "                    if i == num_text_boxes-2:\n",
    "                        composed_text_idxs.append([i+1])\n",
    "            else:\n",
    "                if i != 0 and i not in composed_text_idxs[-1]:\n",
    "                    composed_text_idxs.append([i])\n",
    "                if i == (num_text_boxes-2):\n",
    "                    composed_text_idxs.append([i+1])\n",
    "        return composed_text_idxs\n",
    "\n",
    "def merge_bounding_boxes(composed_text_idxs, bboxes):\n",
    "        '''\n",
    "            openCV --> x: left-to-right, y: top--to-bottom\n",
    "            bbox coordinates --> [[256.0, 1105.0], [1027.0, 1105.0], [1027.0, 1142.0], [256.0, 1142.0]]\n",
    "                             --> left top, right top, right bottom, left bottom\n",
    "\n",
    "            TODO: Also return color palettes for each merged box.\n",
    "        '''\n",
    "        \n",
    "        biggest_borders = []\n",
    "        for idxs in composed_text_idxs:\n",
    "            smallest_x = smallest_y = 10000\n",
    "            biggest_y = biggest_x = 0\n",
    "            if len(idxs) > 1:\n",
    "                for idx in idxs:\n",
    "                    bbox = bboxes[idx]\n",
    "                    bbox_smallest_x, bbox_smallest_y = np.min(bbox, axis=0)\n",
    "                    bbox_biggest_x, bbox_biggest_y = np.max(bbox, axis=0)\n",
    "\n",
    "                    if smallest_x > bbox_smallest_x:\n",
    "                        smallest_x = bbox_smallest_x\n",
    "                    if smallest_y > bbox_smallest_y:\n",
    "                        smallest_y = bbox_smallest_y\n",
    "                    if biggest_x < bbox_biggest_x:\n",
    "                        biggest_x = bbox_biggest_x\n",
    "                    if biggest_y < bbox_biggest_y:\n",
    "                        biggest_y =  bbox_biggest_y\n",
    "\n",
    "                biggest_border = [[smallest_x, smallest_y], [biggest_x, smallest_y], [biggest_x, biggest_y], [smallest_x, biggest_y]]\n",
    "                biggest_borders.append(biggest_border)\n",
    "            else:\n",
    "                print(idxs)\n",
    "                print(idxs[0])\n",
    "                print(bboxes)\n",
    "                print(bboxes[idxs[0]])\n",
    "                biggest_borders.append(bboxes[idxs[0]])\n",
    "        return biggest_borders\n",
    "\n",
    "def extract_text_bbox(ocr, img_path, preview_image_path):\n",
    "        '''\n",
    "            Input: path to the text image\n",
    "            Extract text using paddleOCR.\n",
    "            Crop text from bounding box.\n",
    "            Extract colors using Kmeans inside the bbox.\n",
    "            Return the dominant color and the position.\n",
    "            \n",
    "            DONE: Try to combine very close lines as paragraph bbox. \n",
    "            If the the distance between two bbox is smaller than the bbox height and color is the same,\n",
    "            we can group them as paragraphs.\n",
    "\n",
    "            TODO: Cut images automatically from the sides by a margin.\n",
    "            When constructing bounding boxes, add these margins back to the coordinates.\n",
    "            Sometimes texts are extremely small that the model cannot detect.\n",
    "\n",
    "            Return: text color palettes, dominant colors for each text and position list (as bboxes).\n",
    "        '''\n",
    "        # Parameters for KMeans.\n",
    "        n_colors = 3\n",
    "\n",
    "        result = ocr.ocr(img_path, cls=True)[0]\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        boxes = [line[0] for line in result]\n",
    "        texts = [line[1][0] for line in result]\n",
    "        image = cv2.imread(img_path)\n",
    "        preview_image = cv2.imread(preview_image_path)\n",
    "\n",
    "        palettes = []\n",
    "        dominants = []\n",
    "        new_bboxes = []\n",
    "\n",
    "        # Run KMeans for each text object\n",
    "        for bbox in boxes:\n",
    "            # Crop the text area\n",
    "            x, y = int(bbox[0][0]), int(bbox[0][1])\n",
    "            z, t = int(bbox[2][0]), int(bbox[2][1])\n",
    "            cropped_image = image[y:t, x:z]\n",
    "\n",
    "            # Do template matching to find the places at the actual image because not every image has the same size.\n",
    "            method = cv2.TM_SQDIFF_NORMED\n",
    "            result = cv2.matchTemplate(cropped_image, preview_image, method)\n",
    "            mn,_,mnLoc,_ = cv2.minMaxLoc(result)\n",
    "            MPx,MPy = mnLoc\n",
    "            trows,tcols = cropped_image.shape[:2]\n",
    "            # --> left top, right top, right bottom, left bottom\n",
    "            bbox = [[MPx,MPy], [MPx+tcols, MPy], [MPx+tcols, MPy+trows], [MPx, MPy+trows]]\n",
    "\n",
    "            # Apply KMeans to the text area\n",
    "            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, .1)\n",
    "            flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "\n",
    "            pixels = np.float32(cropped_image.reshape(-1, 3))\n",
    "            _, labels, palette = cv2.kmeans(pixels, n_colors, None, criteria, 10, flags)\n",
    "            palette = np.asarray(palette, dtype=np.int64)\n",
    "            palette_w_white = []\n",
    "\n",
    "            for i, color in enumerate(palette):\n",
    "                x, y, z = color\n",
    "                # Do not add white to the palette since it is the same background in every pic.\n",
    "                if not (252 < x < 256 and 252 < y < 256 and 252 < z < 256):\n",
    "                    palette_w_white.append(color)\n",
    "                else:\n",
    "                    labels = np.delete(labels, np.where(labels == i))\n",
    "\n",
    "            _, counts = np.unique(labels, return_counts=True)\n",
    "            dominant = palette_w_white[np.argmax(counts)]\n",
    "            palettes.append(palette_w_white)\n",
    "            dominants.append(dominant)\n",
    "            new_bboxes.append(bbox)\n",
    "\n",
    "        return palettes, dominants, boxes, texts\n",
    "        \n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def extract_text_directly(ocr, img_path, white_bg_texts):\n",
    "    n_colors = 3\n",
    "    result = ocr.ocr(img_path, cls=True)[0]\n",
    "\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    boxes = [line[0] for line in result]\n",
    "    texts = [line[1][0].replace(\" \", \"\").lower() for line in result]\n",
    "    white_bg_texts = [elem.replace(\" \", \"\").lower() for elem in white_bg_texts]\n",
    "    print(texts, white_bg_texts)\n",
    "    # print(\"My text length\")\n",
    "    # print(len(texts), len(white_bg_texts))\n",
    "    # print(texts, white_bg_texts)\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    same_idxs = []\n",
    "    new_boxes = []\n",
    "    \n",
    "    for j, elem in enumerate(white_bg_texts):\n",
    "        for i, text in enumerate(texts):\n",
    "            if similar(elem, text) > 0.85:\n",
    "                new_boxes.append(boxes[i])\n",
    "            print(\"ifff\")\n",
    "            print(i+1, len(texts))\n",
    "            if i+1 != len(texts):\n",
    "                if similar(elem, text + texts[i+1]) > 0.85:\n",
    "                    # merge boxes\n",
    "                    bboxes = [boxes[i], boxes[i+1]]\n",
    "                    smallest_x = 1000\n",
    "                    smallest_x = smallest_y = 10000\n",
    "                    biggest_y = biggest_x = 0\n",
    "                    for idx in [0, 1]:\n",
    "                        bbox = bboxes[idx]\n",
    "                        bbox_smallest_x, bbox_smallest_y = np.min(bbox, axis=0)\n",
    "                        bbox_biggest_x, bbox_biggest_y = np.max(bbox, axis=0)\n",
    "\n",
    "                        if smallest_x > bbox_smallest_x:\n",
    "                            smallest_x = bbox_smallest_x\n",
    "                        if smallest_y > bbox_smallest_y:\n",
    "                            smallest_y = bbox_smallest_y\n",
    "                        if biggest_x < bbox_biggest_x:\n",
    "                            biggest_x = bbox_biggest_x\n",
    "                        if biggest_y < bbox_biggest_y:\n",
    "                            biggest_y =  bbox_biggest_y\n",
    "\n",
    "                    biggest_border = [[smallest_x, smallest_y], [biggest_x, smallest_y], [biggest_x, biggest_y], [smallest_x, biggest_y]]\n",
    "                    new_boxes.append(biggest_border)                \n",
    "\n",
    "\n",
    "    for bbox in new_boxes:\n",
    "        x, y = int(bbox[0][0]), int(bbox[0][1])\n",
    "        z, t = int(bbox[2][0]), int(bbox[2][1])\n",
    "        cropped_image = image[y:t, x:z]\n",
    "\n",
    "    return new_boxes\n",
    "\n",
    "def extract_decor_elements(decoration_path, preview_path):\n",
    "        # Determine the number of dominant colors\n",
    "        num_colors = 6\n",
    "        \n",
    "        # Load the image\n",
    "        image = cv2.imread(decoration_path)\n",
    "        preview_image = cv2.imread(preview_path)\n",
    "        \n",
    "        # Convert the image to the RGB color space\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image2 = image.copy()\n",
    "        \n",
    "        # Reshape the image to a 2D array of pixels\n",
    "        pixels = image.reshape(-1, 3)\n",
    "        \n",
    "        # Apply K-means clustering with the determined number of colors\n",
    "        kmeans = KMeans(n_clusters=num_colors)\n",
    "        kmeans.fit(pixels)\n",
    "        \n",
    "        # Get the RGB values of the dominant colors\n",
    "        colors = kmeans.cluster_centers_.astype(int)\n",
    "        print(\"Num of colors: \", len(colors))\n",
    "        \n",
    "        # Convert the colors to the HSV color space\n",
    "        hsv_colors = [] \n",
    "\n",
    "        for i, color in enumerate(colors):\n",
    "            x, y, z = color\n",
    "            if not (252 < x < 256 and 252 < y < 256 and 252 < z < 256):\n",
    "                x, y, z = rgb_to_hsv([x/255, y/255, z/255])\n",
    "                hsv_colors.append([x*180, y*255, z*255])\n",
    "        # Convert the image to the HSV color space\n",
    "        hsv_image = cv2.cvtColor(image2, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # Create masks for each dominant color\n",
    "        masks = []\n",
    "        hsv_colors = np.asarray(hsv_colors, dtype=np.int32)\n",
    "        \n",
    "        colors = []\n",
    "        for i in range(len(hsv_colors)):\n",
    "            \n",
    "            h, s, v = hsv_colors[i, :]\n",
    "            lower_color = hsv_colors[i, :] - np.array([10, 50, 50])\n",
    "            upper_color = hsv_colors[i, :] + np.array([10, 255, 255])\n",
    "            mask = cv2.inRange(hsv_image, lower_color, upper_color)\n",
    "            colors.append([h,s,v])\n",
    "            masks.append(mask)\n",
    "        \n",
    "        # Find contours in each mask\n",
    "        contours = []\n",
    "        for mask in masks:\n",
    "            contours_color, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "            contours.append(contours_color)\n",
    "        \n",
    "        # Draw bounding boxes around the shapes\n",
    "        image_with_boxes = image.copy()\n",
    "        bboxes = []\n",
    "        for i, contour_color in enumerate(contours):\n",
    "            for contour in contour_color:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                # left top, right top, right bottom, left bottom\n",
    "                bboxes.append([[x,y], [x+w, y], [x+w, y+h], [x,y+h]])\n",
    "\n",
    "        new_bboxes = NMS(np.asarray(bboxes))\n",
    "        correct_bboxes = []\n",
    "        border_size=1\n",
    "        for bbox in new_bboxes:\n",
    "            [[x,y], [z, y], [z, t], [x, t]] = bbox\n",
    "            #cv2.rectangle(image_with_boxes, (x, y), (z, t), (0, 255, 0), 2)\n",
    "            cropped_image = image[y:t, x:z]\n",
    "            cropped_image = cv2.copyMakeBorder(\n",
    "                cropped_image,\n",
    "                top=border_size,\n",
    "                bottom=border_size,\n",
    "                left=border_size,\n",
    "                right=border_size,\n",
    "                borderType=cv2.BORDER_CONSTANT,\n",
    "                value=[255, 255, 255]\n",
    "            )\n",
    "\n",
    "            method = cv2.TM_SQDIFF_NORMED\n",
    "            result = cv2.matchTemplate(cropped_image, preview_image, method)\n",
    "            mn,_,mnLoc,_ = cv2.minMaxLoc(result)\n",
    "            MPx,MPy = mnLoc\n",
    "            trows,tcols = cropped_image.shape[:2]\n",
    "            bbox = [[MPx+1,MPy-1], [MPx+tcols+1,MPy-1], [MPx+tcols+1, MPy+trows-1], [MPx+1, MPy+trows-1]]\n",
    "            correct_bboxes.append(bbox)\n",
    "        \n",
    "        return colors, bboxes\n",
    "\n",
    "def map_decoration_coordinates(design_text_coordinate, text_coordinate, decoration_coordinates, prev_size, text_size):\n",
    "    # --> [[256.0, 1105.0], [1027.0, 1105.0], [1027.0, 1142.0], [256.0, 1142.0]]\n",
    "    # --> left top, right top, right bottom, left bottom\n",
    "\n",
    "    prev_x, prev_y = prev_size\n",
    "    print(prev_size, text_size)\n",
    "    text_x, text_y = text_size\n",
    "\n",
    "    design_x, design_y = design_text_coordinate[0]\n",
    "    text_x, text_y = text_coordinate[0]\n",
    "    print(\"TEXT X\")\n",
    "    print(text_x)\n",
    "    print(\"DESIGN X\")\n",
    "    print(design_x)\n",
    "\n",
    "    diff_x = text_x - design_x\n",
    "    diff_y = text_y - design_y\n",
    "    \n",
    "    new_coordinates = []\n",
    "    for coordinate in decoration_coordinates:\n",
    "        new_coor = []\n",
    "        for elem in coordinate:\n",
    "            print(\"ELEM X\")\n",
    "            print(elem[0])\n",
    "            new_coor.append([elem[0]-diff_x, elem[1]-diff_y])\n",
    "        new_coordinates.append(new_coor)\n",
    "\n",
    "    return new_coordinates\n",
    "\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='en') # need to run only once to download and load model into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023/07/06 15:59:29] ppocr DEBUG: dt_boxes num : 8, elapse : 0.7562708854675293\n",
      "[2023/07/06 15:59:29] ppocr DEBUG: cls num  : 8, elapse : 0.08286690711975098\n",
      "[2023/07/06 15:59:30] ppocr DEBUG: rec_res num  : 8, elapse : 1.713494062423706\n",
      "8\n",
      "[0]\n",
      "0\n",
      "[[[1303.0, 564.0], [1891.0, 574.0], [1890.0, 633.0], [1302.0, 623.0]], [[1361.0, 643.0], [1838.0, 636.0], [1839.0, 692.0], [1361.0, 699.0]], [[815.0, 990.0], [1601.0, 990.0], [1601.0, 1220.0], [815.0, 1220.0]], [[1838.0, 987.0], [2410.0, 987.0], [2410.0, 1226.0], [1838.0, 1226.0]], [[1848.0, 1309.0], [2406.0, 1301.0], [2407.0, 1393.0], [1849.0, 1400.0]], [[1562.0, 1485.0], [2400.0, 1492.0], [2400.0, 1535.0], [1562.0, 1528.0]], [[1312.0, 1544.0], [2403.0, 1544.0], [2403.0, 1587.0], [1312.0, 1587.0]], [[1478.0, 1597.0], [2400.0, 1597.0], [2400.0, 1639.0], [1478.0, 1639.0]]]\n",
      "[[1303.0, 564.0], [1891.0, 574.0], [1890.0, 633.0], [1302.0, 623.0]]\n",
      "[1]\n",
      "1\n",
      "[[[1303.0, 564.0], [1891.0, 574.0], [1890.0, 633.0], [1302.0, 623.0]], [[1361.0, 643.0], [1838.0, 636.0], [1839.0, 692.0], [1361.0, 699.0]], [[815.0, 990.0], [1601.0, 990.0], [1601.0, 1220.0], [815.0, 1220.0]], [[1838.0, 987.0], [2410.0, 987.0], [2410.0, 1226.0], [1838.0, 1226.0]], [[1848.0, 1309.0], [2406.0, 1301.0], [2407.0, 1393.0], [1849.0, 1400.0]], [[1562.0, 1485.0], [2400.0, 1492.0], [2400.0, 1535.0], [1562.0, 1528.0]], [[1312.0, 1544.0], [2403.0, 1544.0], [2403.0, 1587.0], [1312.0, 1587.0]], [[1478.0, 1597.0], [2400.0, 1597.0], [2400.0, 1639.0], [1478.0, 1639.0]]]\n",
      "[[1361.0, 643.0], [1838.0, 636.0], [1839.0, 692.0], [1361.0, 699.0]]\n",
      "[4]\n",
      "4\n",
      "[[[1303.0, 564.0], [1891.0, 574.0], [1890.0, 633.0], [1302.0, 623.0]], [[1361.0, 643.0], [1838.0, 636.0], [1839.0, 692.0], [1361.0, 699.0]], [[815.0, 990.0], [1601.0, 990.0], [1601.0, 1220.0], [815.0, 1220.0]], [[1838.0, 987.0], [2410.0, 987.0], [2410.0, 1226.0], [1838.0, 1226.0]], [[1848.0, 1309.0], [2406.0, 1301.0], [2407.0, 1393.0], [1849.0, 1400.0]], [[1562.0, 1485.0], [2400.0, 1492.0], [2400.0, 1535.0], [1562.0, 1528.0]], [[1312.0, 1544.0], [2403.0, 1544.0], [2403.0, 1587.0], [1312.0, 1587.0]], [[1478.0, 1597.0], [2400.0, 1597.0], [2400.0, 1639.0], [1478.0, 1639.0]]]\n",
      "[[1848.0, 1309.0], [2406.0, 1301.0], [2407.0, 1393.0], [1849.0, 1400.0]]\n",
      "[2023/07/06 15:59:39] ppocr DEBUG: dt_boxes num : 16, elapse : 0.7780618667602539\n",
      "[2023/07/06 15:59:39] ppocr DEBUG: cls num  : 16, elapse : 0.13921475410461426\n",
      "[2023/07/06 15:59:41] ppocr DEBUG: rec_res num  : 16, elapse : 2.1093099117279053\n",
      "['wednesday25', ':19:oohrs', 'coffee', 'talk', 'joinus', 'loremipsumdolorsitamet', 'consecteturadipisicingelit,seddo', 'eiusmodtemporvelitosquiui', 'f', 'your_account', 'your_account', 'your_account'] ['wednesday25', 'o19:oohrs.', 'coffee', 'talk', 'joinus', 'loremipsumdolorsitamet,', 'consecteturadipisicingelit,seddo', 'eiusmodtemporvelitosquiui.']\n",
      "ifff\n",
      "1 12\n",
      "ifff\n",
      "2 12\n",
      "ifff\n",
      "3 12\n",
      "ifff\n",
      "4 12\n",
      "ifff\n",
      "5 12\n",
      "ifff\n",
      "6 12\n",
      "ifff\n",
      "7 12\n",
      "ifff\n",
      "8 12\n",
      "ifff\n",
      "9 12\n",
      "ifff\n",
      "10 12\n",
      "ifff\n",
      "11 12\n",
      "ifff\n",
      "12 12\n",
      "ifff\n",
      "1 12\n",
      "ifff\n",
      "2 12\n",
      "ifff\n",
      "3 12\n",
      "ifff\n",
      "4 12\n",
      "ifff\n",
      "5 12\n",
      "ifff\n",
      "6 12\n",
      "ifff\n",
      "7 12\n",
      "ifff\n",
      "8 12\n",
      "ifff\n",
      "9 12\n",
      "ifff\n",
      "10 12\n",
      "ifff\n",
      "11 12\n",
      "ifff\n",
      "12 12\n",
      "ifff\n",
      "1 12\n",
      "ifff\n",
      "2 12\n",
      "ifff\n",
      "3 12\n",
      "ifff\n",
      "4 12\n",
      "ifff\n",
      "5 12\n",
      "ifff\n",
      "6 12\n",
      "ifff\n",
      "7 12\n",
      "ifff\n",
      "8 12\n",
      "ifff\n",
      "9 12\n",
      "ifff\n",
      "10 12\n",
      "ifff\n",
      "11 12\n",
      "ifff\n",
      "12 12\n",
      "ifff\n",
      "1 12\n",
      "ifff\n",
      "2 12\n",
      "ifff\n",
      "3 12\n",
      "ifff\n",
      "4 12\n",
      "ifff\n",
      "5 12\n",
      "ifff\n",
      "6 12\n",
      "ifff\n",
      "7 12\n",
      "ifff\n",
      "8 12\n",
      "ifff\n",
      "9 12\n",
      "ifff\n",
      "10 12\n",
      "ifff\n",
      "11 12\n",
      "ifff\n",
      "12 12\n",
      "ifff\n",
      "1 12\n",
      "ifff\n",
      "2 12\n",
      "ifff\n",
      "3 12\n",
      "ifff\n",
      "4 12\n",
      "ifff\n",
      "5 12\n",
      "ifff\n",
      "6 12\n",
      "ifff\n",
      "7 12\n",
      "ifff\n",
      "8 12\n",
      "ifff\n",
      "9 12\n",
      "ifff\n",
      "10 12\n",
      "ifff\n",
      "11 12\n",
      "ifff\n",
      "12 12\n",
      "ifff\n",
      "1 12\n",
      "ifff\n",
      "2 12\n",
      "ifff\n",
      "3 12\n",
      "ifff\n",
      "4 12\n",
      "ifff\n",
      "5 12\n",
      "ifff\n",
      "6 12\n",
      "ifff\n",
      "7 12\n",
      "ifff\n",
      "8 12\n",
      "ifff\n",
      "9 12\n",
      "ifff\n",
      "10 12\n",
      "ifff\n",
      "11 12\n",
      "ifff\n",
      "12 12\n",
      "ifff\n",
      "1 12\n",
      "ifff\n",
      "2 12\n",
      "ifff\n",
      "3 12\n",
      "ifff\n",
      "4 12\n",
      "ifff\n",
      "5 12\n",
      "ifff\n",
      "6 12\n",
      "ifff\n",
      "7 12\n",
      "ifff\n",
      "8 12\n",
      "ifff\n",
      "9 12\n",
      "ifff\n",
      "10 12\n",
      "ifff\n",
      "11 12\n",
      "ifff\n",
      "12 12\n",
      "ifff\n",
      "1 12\n",
      "ifff\n",
      "2 12\n",
      "ifff\n",
      "3 12\n",
      "ifff\n",
      "4 12\n",
      "ifff\n",
      "5 12\n",
      "ifff\n",
      "6 12\n",
      "ifff\n",
      "7 12\n",
      "ifff\n",
      "8 12\n",
      "ifff\n",
      "9 12\n",
      "ifff\n",
      "10 12\n",
      "ifff\n",
      "11 12\n",
      "ifff\n",
      "12 12\n",
      "8\n",
      "[0]\n",
      "0\n",
      "[[[897.0, 137.0], [1479.0, 146.0], [1478.0, 200.0], [897.0, 192.0]], [[407.0, 560.0], [1173.0, 560.0], [1173.0, 791.0], [407.0, 791.0]], [[1439.0, 555.0], [1992.0, 555.0], [1992.0, 797.0], [1439.0, 797.0]], [[1426.0, 877.0], [1997.0, 872.0], [1997.0, 960.0], [1427.0, 966.0]], [[1158.0, 872.0], [1997.0, 872.0], [1997.0, 1100.0], [1158.0, 1100.0]], [[1158.0, 1064.0], [1984.0, 1064.0], [1984.0, 1100.0], [1158.0, 1100.0]], [[905.0, 1118.0], [1987.0, 1118.0], [1987.0, 1155.0], [905.0, 1155.0]], [[1069.0, 1170.0], [1984.0, 1170.0], [1984.0, 1207.0], [1069.0, 1207.0]], [[1069.0, 1170.0], [1984.0, 1170.0], [1984.0, 1544.0], [1069.0, 1544.0]]]\n",
      "[[897.0, 137.0], [1479.0, 146.0], [1478.0, 200.0], [897.0, 192.0]]\n",
      "[1]\n",
      "1\n",
      "[[[897.0, 137.0], [1479.0, 146.0], [1478.0, 200.0], [897.0, 192.0]], [[407.0, 560.0], [1173.0, 560.0], [1173.0, 791.0], [407.0, 791.0]], [[1439.0, 555.0], [1992.0, 555.0], [1992.0, 797.0], [1439.0, 797.0]], [[1426.0, 877.0], [1997.0, 872.0], [1997.0, 960.0], [1427.0, 966.0]], [[1158.0, 872.0], [1997.0, 872.0], [1997.0, 1100.0], [1158.0, 1100.0]], [[1158.0, 1064.0], [1984.0, 1064.0], [1984.0, 1100.0], [1158.0, 1100.0]], [[905.0, 1118.0], [1987.0, 1118.0], [1987.0, 1155.0], [905.0, 1155.0]], [[1069.0, 1170.0], [1984.0, 1170.0], [1984.0, 1207.0], [1069.0, 1207.0]], [[1069.0, 1170.0], [1984.0, 1170.0], [1984.0, 1544.0], [1069.0, 1544.0]]]\n",
      "[[407.0, 560.0], [1173.0, 560.0], [1173.0, 791.0], [407.0, 791.0]]\n",
      "[2]\n",
      "2\n",
      "[[[897.0, 137.0], [1479.0, 146.0], [1478.0, 200.0], [897.0, 192.0]], [[407.0, 560.0], [1173.0, 560.0], [1173.0, 791.0], [407.0, 791.0]], [[1439.0, 555.0], [1992.0, 555.0], [1992.0, 797.0], [1439.0, 797.0]], [[1426.0, 877.0], [1997.0, 872.0], [1997.0, 960.0], [1427.0, 966.0]], [[1158.0, 872.0], [1997.0, 872.0], [1997.0, 1100.0], [1158.0, 1100.0]], [[1158.0, 1064.0], [1984.0, 1064.0], [1984.0, 1100.0], [1158.0, 1100.0]], [[905.0, 1118.0], [1987.0, 1118.0], [1987.0, 1155.0], [905.0, 1155.0]], [[1069.0, 1170.0], [1984.0, 1170.0], [1984.0, 1207.0], [1069.0, 1207.0]], [[1069.0, 1170.0], [1984.0, 1170.0], [1984.0, 1544.0], [1069.0, 1544.0]]]\n",
      "[[1439.0, 555.0], [1992.0, 555.0], [1992.0, 797.0], [1439.0, 797.0]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m composed_text_idxs1 \u001b[39m=\u001b[39m compose_paragraphs(text_bboxes1, text_palettes)\n\u001b[1;32m     36\u001b[0m merged_bboxes1 \u001b[39m=\u001b[39m merge_bounding_boxes(composed_text_idxs1, text_bboxes1)\n\u001b[0;32m---> 37\u001b[0m decoration_hsv_palettes, decoration_bboxes \u001b[39m=\u001b[39m extract_decor_elements(decoration, preview)\n\u001b[1;32m     38\u001b[0m new_coordinates \u001b[39m=\u001b[39m map_decoration_coordinates(text_bboxes1[\u001b[39m0\u001b[39m], text_bboxes[\u001b[39m0\u001b[39m], decoration_bboxes, (image\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], image\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]), (image_text\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], image_text\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m bbox \u001b[39min\u001b[39;00m merged_bboxes1:\n",
      "Cell \u001b[0;32mIn [37], line 241\u001b[0m, in \u001b[0;36mextract_decor_elements\u001b[0;34m(decoration_path, preview_path)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39m# Apply K-means clustering with the determined number of colors\u001b[39;00m\n\u001b[1;32m    240\u001b[0m kmeans \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39mnum_colors)\n\u001b[0;32m--> 241\u001b[0m kmeans\u001b[39m.\u001b[39;49mfit(pixels)\n\u001b[1;32m    243\u001b[0m \u001b[39m# Get the RGB values of the dominant colors\u001b[39;00m\n\u001b[1;32m    244\u001b[0m colors \u001b[39m=\u001b[39m kmeans\u001b[39m.\u001b[39mcluster_centers_\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/causalml-py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1068\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1064\u001b[0m seeds \u001b[39m=\u001b[39m random_state\u001b[39m.\u001b[39mrandint(np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax, size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_init)\n\u001b[1;32m   1066\u001b[0m \u001b[39mfor\u001b[39;00m seed \u001b[39min\u001b[39;00m seeds:\n\u001b[1;32m   1067\u001b[0m     \u001b[39m# run a k-means once\u001b[39;00m\n\u001b[0;32m-> 1068\u001b[0m     labels, inertia, centers, n_iter_ \u001b[39m=\u001b[39m kmeans_single(\n\u001b[1;32m   1069\u001b[0m         X, sample_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_clusters, max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m   1070\u001b[0m         init\u001b[39m=\u001b[39;49minit, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tol,\n\u001b[1;32m   1071\u001b[0m         x_squared_norms\u001b[39m=\u001b[39;49mx_squared_norms, random_state\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m   1072\u001b[0m         n_threads\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_n_threads)\n\u001b[1;32m   1073\u001b[0m     \u001b[39m# determine if these results are the best so far\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m     \u001b[39mif\u001b[39;00m best_inertia \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m inertia \u001b[39m<\u001b[39m best_inertia:\n",
      "File \u001b[0;32m~/miniconda3/envs/causalml-py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:415\u001b[0m, in \u001b[0;36m_kmeans_single_elkan\u001b[0;34m(X, sample_weight, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, n_threads)\u001b[0m\n\u001b[1;32m    412\u001b[0m     elkan_iter \u001b[39m=\u001b[39m elkan_iter_chunked_dense\n\u001b[1;32m    413\u001b[0m     _inertia \u001b[39m=\u001b[39m _inertia_dense\n\u001b[0;32m--> 415\u001b[0m init_bounds(X, centers, center_half_distances,\n\u001b[1;32m    416\u001b[0m             labels, upper_bounds, lower_bounds)\n\u001b[1;32m    418\u001b[0m strict_convergence \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iter):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DETECT TEXT ON TEXT IMAGE\n",
    "text = '../destijl_dataset/04_text/0328.png'\n",
    "preview = '../destijl_dataset/00_preview/0328.png'\n",
    "decoration = '../destijl_dataset/03_decoration/0328.png'\n",
    "image = cv2.imread(preview)\n",
    "image_text = cv2.imread(text)\n",
    "image2 = image.copy()\n",
    "\n",
    "text_palettes, text_dominants, text_bboxes, texts = extract_text_bbox(ocr, text, preview)\n",
    "\n",
    "composed_text_idxs = compose_paragraphs(text_bboxes, text_palettes)\n",
    "merged_bboxes = merge_bounding_boxes(composed_text_idxs, text_bboxes)\n",
    "\n",
    "# DETECT TEXT ON REAL DESIGN\n",
    "image = cv2.imread(preview)\n",
    "image2 = image.copy()\n",
    "image3 = image.copy()\n",
    "\n",
    "text_bboxes1 = extract_text_directly(ocr, preview, texts)\n",
    "\n",
    "for bbox in text_bboxes1:\n",
    "    x, y = bbox[0][0], bbox[0][1]\n",
    "    z, t = bbox[2][0], bbox[2][1]\n",
    "    cv2.rectangle(image, (int(x), int(y)), (int(z), int(t)), (0, 255, 0), 2)\n",
    "    \n",
    "cv2.imwrite('result_prev.jpg', image)\n",
    "\n",
    "for bbox in text_bboxes:\n",
    "    x, y = bbox[0][0], bbox[0][1]\n",
    "    z, t = bbox[2][0], bbox[2][1]\n",
    "    cv2.rectangle(image2, (int(x), int(y)), (int(z), int(t)), (0, 255, 0), 2)\n",
    "    \n",
    "cv2.imwrite('result_text.jpg', image2)\n",
    "\n",
    "composed_text_idxs1 = compose_paragraphs(text_bboxes1, text_palettes)\n",
    "merged_bboxes1 = merge_bounding_boxes(composed_text_idxs1, text_bboxes1)\n",
    "decoration_hsv_palettes, decoration_bboxes = extract_decor_elements(decoration, preview)\n",
    "new_coordinates = map_decoration_coordinates(text_bboxes1[0], text_bboxes[0], decoration_bboxes, (image.shape[0], image.shape[1]), (image_text.shape[0], image_text.shape[1]))\n",
    "\n",
    "\n",
    "for bbox in merged_bboxes1:\n",
    "    x, y = bbox[0][0], bbox[0][1]\n",
    "    z, t = bbox[2][0], bbox[2][1]\n",
    "    cv2.rectangle(image3, (int(x), int(y)), (int(z), int(t)), (0, 255, 0), 2)\n",
    "    \n",
    "cv2.imwrite('result_dec.jpg', image3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('causalml-py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fe1114d0d27f71ad4119f0373fc1aaac921ee939a643d798102047e2deae920"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
